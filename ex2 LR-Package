# Logistic regression and Regulation
# 在訓練的初始階段，構建一個邏輯回歸模型來預測，某位學生是否被大學錄取。假設你是大學招生官，想通過申請的學生兩次測試來評分，決定是否被錄取。
# 有申請學生的訓練樣本集將其用於訓練邏輯回歸，每一個訓練樣本，有他們兩次測試的評分和最後是錄取的結果。
#直接調用資料庫。這就是不用自己定義迭代次數和步長，就會直接給出最優解。 (Python可用scipy.optimize.fmin_tnc)
# andrew ng在課程中用的是Octave的“fminunc”函數
import scipy.optimize as opt
result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, y))
result
#求出cost function
cost(result[0], X, y)
#畫出分隔線圖
plotting_x1 = np.linspace(30, 100, 100)
plotting_h1 = ( - result[0][0] - result[0][1] * plotting_x1) / result[0][2]

fig, ax = plt.subplots(figsize=(12,8))
ax.plot(plotting_x1, plotting_h1, 'y', label='Prediction')
ax.scatter(positive['Exam 1'], positive['Exam 2'], s=50, c='b', marker='o', label='Admitted')
ax.scatter(negative['Exam 1'], negative['Exam 2'], s=50, c='r', marker='x', label='Not Admitted')
ax.legend()
ax.set_xlabel('Exam 1 Score')
ax.set_ylabel('Exam 2 Score')
plt.show()
# predict 學生是否錄取
def hfunc1(theta, X):
    return sigmoid(np.dot(theta.T, X))
hfunc1(result[0],[1,45,85])
# 預測準確率
theta_min = np.matrix(result[0])
predictions = predict(theta_min, X)
correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)]
accuracy = (sum(map(int, correct)) % len(correct))
print ('accuracy = {0}%'.format(accuracy))
